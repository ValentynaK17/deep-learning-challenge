# deep-learning-challenge
Binary classifier, based on neural networks, which helps to predict the applicants for funding with the best chance of success. <br>
## Overview
A nonprofit organization needs to improve a process for selecting of the applicants for funding. <br>
Having:
 - that the main criteria for selection is a probability of the applicant to success in their projects,
 - [historical data](https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv) of more than 34,000 organizations that have received funding from the organization over the years, provided by a business team.

We can help a business to predict whether applicants will be successful if funded by the organization, using machine learning and neural networks.

## Results
#### Data Preprocessing
We are trying to predict a success of the applicant, so our target variable is ‘IS_SUCCESSFUL’ value.
The success of the applicant is dependent from such things as:
 - Applicant itself,
 - Type of the application,
 - Affiliated sector of industry and Organization type,
 - Such independent classifications as: Use case for funding, Government organization classification, Income range,
 - Amount of funding requested,
 - Whether there were any special considerations for application.
     
So, as features we have the following columns: *NAME*, *APPLICATION_TYPE*, *AFFILIATION*, *CLASSIFICATION*, *USE_CASE*, *ORGANIZATION*, *STATUS*, *INCOME_AMT*, *SPECIAL_CONSIDERATIONS* and *ASK_AMT*. <br>
Data associated with 'EIN' and 'STATUS' were ignored, because: 
 1. EIN is just an id in the internal system without any impact on success of the applicant,
 2. 'STATUS' column has same value for all the rows, that is why is not really informative for predictions.<br>
#### Compiling, Training, and Evaluating the Model
The structure of final neural network model, which showed best result is the following:
| Layer | Number of Neurons| Activation Function|
|----------|----------|----------|
|Input| -  |  -  |
|Initial Layer| 40 | ‘tanh’|
|Hidden Layer| 16 | ‘tanh’|
|Hidden Layer| 6 | ‘tanh’|
|Output Layer| 1 | 'sigmoid'|

It was created by having initial set of hyperparameters generated by keras tuner as a starting point with few adjustments to:
 - Number of layers: by experimenting with hidden layers it was concluded that having 4 layers instead of suggested 3 gave better results. More than 4 layers didn’t led to better metrics. So 4 layers was a choice.
 - Activation function: ‘tanh’ usually gave better results for the same rest parameters. So, ‘tanh’ was selected.
Starting from accuracy equal to 73% the output was improved to accuracy 76% after applying all the optimizations:
<p align="center">
<img src="https://github.com/ValentynaK17/deep-learning-challenge/blob/main/Outputs/Metrics_Final.png" width=“275">
</p>
History of training for the best resulted model:
<p align="center">
<img src="https://github.com/ValentynaK17/deep-learning-challenge/blob/main/Outputs/History_Final_Tuning.png" width=“275">
</p>

In order to increase model performance the next improvements were made:<br>
 - removed column that didn’t affect the success of applicants,
 - added a column that have an impact on success of applicants,
 - adjusted binning, by adding extra bins for categorical data,
 - introduced oversampler,
 - added more hidden layers,
 - adjusted neurons count in hidden layers,
 - adjusted activation function,
 - selected epochs count based on metrics values analysis depending on higher and lower epochs count.

## Summary
Helping to improve the selection process for funding for a nonprofit organization, a deep learning model was created using historical data from over 34,000 funded by the organization projects. The model predicts success of applicants, excluding not informative data like 'EIN' and 'STATUS'. <br>
After a series of optimizations, including conversion of categorical data into numeric and adjustments of hyperparameters, the model accuracy was optimized from 73% to 76%. <br>
The training history shows a reduction in model’s loss and increase in its accuracy over 35 epochs.<br>
As an alternative, for predictions, we can use here:
- *k-nearest neighbors*: relatevily simple model, which can help with predictions for non-linear relationships between features like we have is our case. However, it could be slow having large number of features. 
- *Tree-based Models*: Decision Trees (or Random Forest for better output) are good alternatives here, as they can be used for categorization with higher dimensionality. However, due to tendency to overfitting, the pruning may need to be applied here.

### Repository Contents
 - **Binary_Classification.ipynb** initial scripts for prediction of the applicants’ success <br>
 - **AlphabetSoupCharity_Optimization_First_Try.ipynb** a little optimized scripts for the same predictions:
   - Removed ‘STATUS’ column
   - Added more bins for ‘CATEGORY’ column
   - Adjusted approach for ‘INCOME_AMT' column, by to converting it into numbers based on min value of a range 
   - Introduced oversampler
 - **AlphabetSoupCharity_Optimization_Second_Try.ipynb**  are the scripts with few more optimizations for prediction of the applicants’ success: <br>
   - Added ‘NAME’ column
   - Adjusted number of hidden layers, and neurons as well as epochs
 - **AlphabetSoupCharity_Optimization_Final.ipynb**  the *final* scripts with extra binning and few adjustments in neurons and epochs count
 - **AlphabetSoupCharity.h5** exported very first model with its layers and their order, weights of the model and hyperparameters.
 - **Outputs** directory contains few images with the history of model training and metrics for final model<br>
### Installation
 - Having Python installed on your machine along with Jupyter Notebook available, clone this repository to your local machine <br>
### Usage
 - Run the *AlphabetSoupCharity_Optimization_Final.ipynb* script using Jupyter Notebook OR use *AlphabetSoupCharity.h5* to resume model training<br>
